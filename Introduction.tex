The first companies that took advantage of the Internet to provide web services built their business around small, static server machines with limited computing power that offered little to no flexibility and required the manufacturer to access the remote machine and update it manually when needed. Hence, after years of improvements and solutions in favour of higher flexibility, the research and development direction aligned toward new technologies, such as the Cloud.
The Cloud computing paradigm represented a true innovation for web services providers since it granted virtually infinite resources, an excellent and maintained complete infrastructure, the remote management of the rented machines and different payment options that allowed customers to have more or fewer responsibilities over their systems. In addition, the dynamicity provided by the Cloud allowed for more flexible designs for web services and started a research path that inevitably led to more dynamic, flexible and pervasive architectures. The first major step in such a direction was toward a computational model closer to the end-users. Many online services started having strict latency requirements that the Cloud could not satisfy due to the physical distance between the server and the users. Furthermore, a new technology started diffusing, the Internet of Things, which involved a huge amount of devices with integrated sensors capable of producing immense moles of data that soon showed the Cloud's resources are finite and limited.

The first solid solution to the abovementioned problems is the Edge computing paradigm, which introduced the concept of computing nodes, small interconnected servers in charge of aggregating and pre-processing the data generated by the adjacent smart devices. Of course, such architecture still relies on the Cloud for the final data processing since the nodes are not powerful enough to do that in real-time, but it considerably simplifies the work for the Cloud.

During all the technological development over the years, one point always stood up as fundamental for the online services providers, the security of such technologies, from the software to the hardware infrastructures. Enterprise web services have faced continuous security threats since they began spreading across the Internet; additionally, web services started integrating other web services unifying the functionalities and features and expanding increasingly. The more integrated services, the heavier the work to ensure the integration was secure for the customers' devices and internal components such as databases and source code; moreover, laws started to become more strict over cyber-security and data protection progressively and started demanding proof of services' security. For these reasons, cyber-security certification schemes were introduced, allowing web services to obtain a certificate they could show proving the security of the system. Besides, such schemes underwent numerous iterations, eventually leading to a relatively low number of internationally accepted standards, the most known being the Common Criteria \cite{infrastructure2002common}. The certification schemes focused on demonstrating the correct behaviour of the various security components of the systems, including, but not limited to, connection protocols, data management systems, encryption algorithms and user privacy policies.
Subsequently, the focus moved from static web services to more dynamic infrastructures, such as the Cloud \cite{mell2011nist}, that needed more automated schemes for the certification processes; many researchers worked on developing new solutions for better certification schemes for the Cloud specifically, such as Anisetti et al. in \cite{anisetti2017semi}, facing the challenges brought up by the new environments, such as i) the contextual changes that might invalidate the traditional certificates, ii) the numerous layers of services (e.g. networking infrastructure, servers, virtual environments, development tools and included software) and iii) the incremental nature of Cloud systems, where new services are continuously added. Dynamicity means flexibility for the end user, and starting from the Cloud, the focus was moved to increasingly dynamic architectures, such as Edge computing and the Internet of Things. Edge (or Fog) computing added a number of challenges to the security assurance of systems; more specifically, the high number of devices with low resources involved was not an easy task to deal with, and it is not a fully mature topic yet. Researchers like Aslam et al. in \cite{aslam2020fonac} addressed some of such issues by introducing less invasive techniques like continuous monitoring and auditing of the Edge nodes. Following the trend of increasing the dynamicity and flexibility of systems, the Internet of Things (IoT) has been ramping up in popularity in recent years, dealing with increasingly sensitive data. IoT systems can reach tens of thousands of interconnected devices, and when such devices handle data that must stay private and protected, they easily become a target for security attacks. The modern approaches to IoT systems certifications still rely on traditional schemes like Common Criteria, the same scheme used for static web services; the issue with standard approaches is that they rely heavily on the staticity of the certified system and do not contemplate changes in the system nor in the context once deployed. Continuously dealing with complete recertifications every time the system undergoes some change is not feasible for the manufacturers for two main reasons: i) the certification process is long, it can last for months and could be triggered by a security patch, leaving the system exposed to a vulnerability for a long time and ii) the process is costly, easily falling into the hundreds of thousands of Euros for high-level certificates. 

The most effective way of developing assurance techniques is often by evolving the current standards, and so it has been, in the certification scheme field, until the IoT paradigm faced researchers with the abovementioned issues. Unfortunately, few efforts have been proposed to overcome the problem, such as [19][20] and [21], but they all rely on the traditional schemes and result in small variations of them, equally incapable of dealing with the dynamic nature of the IoT systems. Hence, the gap in the research is presented as a possible different direction that needs to be taken. Furthermore, IoT systems need space for changes without needing re-evaluation regularly and penalising their functionalities during an evaluation process. Therefore, this thesis is willing to take a first step toward solving the challenges posed by dynamic and pervasive architectures by tackling the following points: i) the certification process is made up of components that never really changed over the years, but in the IoT environment, they need to, ii) the certification schemes usually rely on the complete knowledge of the system, allowing them to precisely craft well-aimed tests over the features of the systems; this is not possible in a system involving thousands of devices and iii) traditional schemes never had to deal with limitations over systems' resources for their processes, with IoT they have to, in order not to clog the entire service. Thus, the goal of the approach proposed in this thesis is to finally have a faster, cheaper, lighter and more automated certification process for highly dynamic systems.

A certification scheme that allows reducing the weight over the system's components through automation and alternative testing techniques, drastically reduces the redundancy, the time and the resources needed to obtain a certificate; such reductions are fundamental for a system that possibly needs to undergo a high number of certification processes.

The methodology used in this thesis to approach our goal mainly focuses on the formal definition of the components that constitute the final certification process. First off, two new elements are added to the traditional set of certification components, the scoring system and the trigger. The scoring system is a feature that allows guiding the process using numerical values (e.g. scores and score thresholds) to evaluate the impact of each systemâ€™s feature in terms of value and effort needed to test it; the trigger is a component whose task is monitoring the system and detecting changes, eventually triggering the certification process and providing the necessary information to complete it. Then, we introduce the new models of the remaining components: i) attributes, ii) properties, iii) evidence collection and iv) certificate; such components needed to be revisited to fit the scheme correctly.

\newpage
The thesis work is structured as follows:
\begin{description}
    \item[State of the Art] Study of the current efforts toward the certification schemes in dynamic systems to better visualize the background and context considered in this thesis;
    \item[Methodology] Introduction of the methodology of the proposed certification scheme over a simple simulated scenario, where the certification components' models and phases are introduced, and the process is simulated, step-by-step, in an IoT smart-city example;
    \item[Experiments] Execution of the proposed certification scheme over a real-world-inspired scenario, where each process step is shown and accurately described, finally leading to the certificate's release.
\end{description}